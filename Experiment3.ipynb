{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn+AgiH2np34KLWVz8CtqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyanshSharma17/Neural_network/blob/master/Experiment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3\n",
        "\n",
        "WAP to implement a three-layer neural network using Tensor flow library (only, no keras)\n",
        "to classify MNIST handwritten digits dataset.\n",
        "Demonstrate the implementation of feed-forward and back-propagation approaches\n",
        "\n",
        "Model Description:\n",
        "\n",
        "This is a simple neural network designed to classify handwritten digits from\n",
        "the MNIST dataset. The model is implemented using TensorFlow 2.x without the use of Keras,\n",
        "giving a hands-on experience with TensorFlow's lower-level operations.\n",
        "The network consists of three layers, and we manually define the feed-forward and\n",
        "backpropagation steps.\n",
        "\n",
        "Model Architecture:\n",
        "\n",
        "The model has three layers:\n",
        "\n",
        "Input Layer: The MNIST images are 28x28 pixels, which are flattened into a 1D vector of 784 values (28 * 28 = 784). This forms the input layer.\n",
        "Hidden Layer: A fully connected (dense) layer with 128 neurons. The activation function used here is ReLU (Rectified Linear Unit), which introduces non-linearity into the model and allows it to learn complex patterns.\n",
        "Output Layer: The final layer is a softmax layer with 10 neurons. Each neuron corresponds to one of the 10 possible digits (0-9). The model outputs raw logits (i.e., unnormalized scores) for each class."
      ],
      "metadata": {
        "id": "YRq6Jv8tWlfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the MNIST dataset using tensorflow_datasets\n",
        "mnist_data, info = tfds.load('mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "# Prepare the training and test sets\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "\n",
        "# Normalize the data\n",
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize the image to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Map the preprocessing function and batch the data\n",
        "train_data = train_data.map(preprocess).batch(128).shuffle(60000).repeat()\n",
        "test_data = test_data.map(preprocess).batch(128)\n",
        "\n",
        "# Define the three-layer neural network model with"
      ],
      "metadata": {
        "id": "mhdTRrhsV4bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreeLayerNN(tf.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize weights and biases\n",
        "        self.w1 = tf.Variable(tf.random.normal([784, 128]))  # Weights for the first layer\n",
        "        self.b1 = tf.Variable(tf.zeros([128]))  # Biases for the first layer\n",
        "        self.w2 = tf.Variable(tf.random.normal([128, 10]))  # Weights for the second layer (output layer)\n",
        "        self.b2 = tf.Variable(tf.zeros([10]))  # Biases for the second layer\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Feed-forward process: Apply the layers and activation functions\n",
        "        x = tf.reshape(x, [-1, 784])  # Flatten input to 1D vector (28x28 = 784)\n",
        "\n",
        "        # First layer (input -> hidden)\n",
        "        hidden = tf.matmul(x, self.w1) + self.b1\n",
        "        hidden = tf.nn.relu(hidden)  # ReLU activation function\n",
        "\n",
        "        # Second layer (hidden -> output)\n",
        "        output = tf.matmul(hidden, self.w2) + self.b2\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = ThreeLayerNN()\n",
        "\n",
        "# Define the loss function\n",
        "def compute_loss(logits, labels):\n",
        "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
        "\n",
        "# Define the optimization step (gradient descent)\n",
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "# Training step\n",
        "def train_step(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images)  # Forward pass\n",
        "        loss = compute_loss(logits, labels)  # Compute the loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)  # Backpropagation\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Update weights\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "max_steps = 6000\n",
        "steps_taken = 0\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for step, (images, labels) in enumerate(train_data):\n",
        "        if steps_taken >= max_steps:\n",
        "            print(f\"Training stopped after {steps_taken} steps.\")\n",
        "            break\n",
        "        loss = train_step(model, images, labels)\n",
        "        total_loss += loss\n",
        "        steps_taken += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / (step+1)}\")\n",
        "    if steps_taken >= max_steps:\n",
        "        break\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate(model, test_data):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for images, labels in test_data:\n",
        "        logits = model(images)\n",
        "        predicted_classes = tf.argmax(logits, axis=1)\n",
        "        correct_predictions += tf.reduce_sum(tf.cast(tf.equal(predicted_classes, labels), tf.int32))\n",
        "        total_predictions += len(labels)\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Test Accuracy: {accuracy.numpy():.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate(model, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV-Sn85EXczT",
        "outputId": "a7afc7cd-fca8-4a0d-eb81-d5191134dd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Step 0, Loss: 126.90067291259766\n",
            "Epoch 1, Step 100, Loss: 27.15457534790039\n",
            "Epoch 1, Step 200, Loss: 11.818521499633789\n",
            "Epoch 1, Step 300, Loss: 16.30792236328125\n",
            "Epoch 1, Step 400, Loss: 5.188839912414551\n",
            "Epoch 1, Step 500, Loss: 3.3088412284851074\n",
            "Epoch 1, Step 600, Loss: 8.113788604736328\n",
            "Epoch 1, Step 700, Loss: 3.8102915287017822\n",
            "Epoch 1, Step 800, Loss: 4.728327751159668\n",
            "Epoch 1, Step 900, Loss: 3.5811080932617188\n",
            "Epoch 1, Step 1000, Loss: 1.6203113794326782\n",
            "Epoch 1, Step 1100, Loss: 2.0492782592773438\n",
            "Epoch 1, Step 1200, Loss: 6.201204299926758\n",
            "Epoch 1, Step 1300, Loss: 3.633856773376465\n",
            "Epoch 1, Step 1400, Loss: 2.5876033306121826\n",
            "Epoch 1, Step 1500, Loss: 2.502194881439209\n",
            "Epoch 1, Step 1600, Loss: 2.4886116981506348\n",
            "Epoch 1, Step 1700, Loss: 1.356600046157837\n",
            "Epoch 1, Step 1800, Loss: 2.093639850616455\n",
            "Epoch 1, Step 1900, Loss: 2.7216124534606934\n",
            "Epoch 1, Step 2000, Loss: 0.9919930100440979\n",
            "Epoch 1, Step 2100, Loss: 2.443976879119873\n",
            "Epoch 1, Step 2200, Loss: 1.629671573638916\n",
            "Epoch 1, Step 2300, Loss: 2.354616165161133\n",
            "Epoch 1, Step 2400, Loss: 1.0574090480804443\n",
            "Epoch 1, Step 2500, Loss: 0.6914865970611572\n",
            "Epoch 1, Step 2600, Loss: 0.672972559928894\n",
            "Epoch 1, Step 2700, Loss: 2.4539546966552734\n",
            "Epoch 1, Step 2800, Loss: 1.6965408325195312\n",
            "Epoch 1, Step 2900, Loss: 0.9150717854499817\n",
            "Epoch 1, Step 3000, Loss: 0.4127340614795685\n",
            "Epoch 1, Step 3100, Loss: 1.759777545928955\n",
            "Epoch 1, Step 3200, Loss: 1.215261697769165\n",
            "Epoch 1, Step 3300, Loss: 0.801301121711731\n",
            "Epoch 1, Step 3400, Loss: 0.42484384775161743\n",
            "Epoch 1, Step 3500, Loss: 0.5068597793579102\n",
            "Epoch 1, Step 3600, Loss: 1.2876222133636475\n",
            "Epoch 1, Step 3700, Loss: 1.3356661796569824\n",
            "Epoch 1, Step 3800, Loss: 0.3825570344924927\n",
            "Epoch 1, Step 3900, Loss: 0.5215803980827332\n",
            "Epoch 1, Step 4000, Loss: 1.3724994659423828\n",
            "Epoch 1, Step 4100, Loss: 0.9827844500541687\n",
            "Epoch 1, Step 4200, Loss: 0.7398185133934021\n",
            "Epoch 1, Step 4300, Loss: 2.2204458713531494\n",
            "Epoch 1, Step 4400, Loss: 0.8470225930213928\n",
            "Epoch 1, Step 4500, Loss: 0.5633662939071655\n",
            "Epoch 1, Step 4600, Loss: 0.8427486419677734\n",
            "Epoch 1, Step 4700, Loss: 0.6500698924064636\n",
            "Epoch 1, Step 4800, Loss: 0.397439569234848\n",
            "Epoch 1, Step 4900, Loss: 0.06377188861370087\n",
            "Epoch 1, Step 5000, Loss: 0.48887720704078674\n",
            "Epoch 1, Step 5100, Loss: 0.3899066746234894\n",
            "Epoch 1, Step 5200, Loss: 0.3333030939102173\n",
            "Epoch 1, Step 5300, Loss: 0.38739442825317383\n",
            "Epoch 1, Step 5400, Loss: 0.2873765528202057\n",
            "Epoch 1, Step 5500, Loss: 0.39702510833740234\n",
            "Epoch 1, Step 5600, Loss: 0.6654716730117798\n",
            "Epoch 1, Step 5700, Loss: 0.3931701183319092\n",
            "Epoch 1, Step 5800, Loss: 0.4279744029045105\n",
            "Epoch 1, Step 5900, Loss: 0.8365904092788696\n",
            "Training stopped after 6000 steps.\n",
            "Epoch 1, Average Loss: 3.1334714889526367\n",
            "Test Accuracy: 0.9324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Performance Evaluation\n",
        "The model steadily reduces loss over training epochs, indicating effective learning.\n",
        "It shows promising accuracy on the MNIST test set, confirming its reliability for digit recognition.\n",
        "Overall, the evaluation reflects a well-optimized training process with space for fine-tuning.\n",
        "\n",
        "My Comments\n",
        "The custom neural network is straightforward yet effective for a classification task.\n",
        "It nicely demonstrates the concepts of feed-forward processing and backpropagation.\n",
        "Minor hyperparameter adjustments could further enhance its performance."
      ],
      "metadata": {
        "id": "VowHLX2Z68IS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}