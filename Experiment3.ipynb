{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment 3\n",
    "\n",
    "WAP to implement a three-layer neural network using Tensor flow library (only, no keras) \n",
    "to classify MNIST handwritten digits dataset. \n",
    "Demonstrate the implementation of feed-forward and back-propagation approaches\n",
    "\n",
    "Model Description:\n",
    "\n",
    "This is a simple neural network designed to classify handwritten digits from \n",
    "the MNIST dataset. The model is implemented using TensorFlow 2.x without the use of Keras, \n",
    "giving a hands-on experience with TensorFlow's lower-level operations.\n",
    "The network consists of three layers, and we manually define the feed-forward and \n",
    "backpropagation steps.\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "The model has three layers:\n",
    "\n",
    "Input Layer: The MNIST images are 28x28 pixels, which are flattened into a 1D vector of 784 values (28 * 28 = 784). This forms the input layer.\n",
    "Hidden Layer: A fully connected (dense) layer with 128 neurons. The activation function used here is ReLU (Rectified Linear Unit), which introduces non-linearity into the model and allows it to learn complex patterns.\n",
    "Output Layer: The final layer is a softmax layer with 10 neurons. Each neuron corresponds to one of the 10 possible digits (0-9). The model outputs raw logits (i.e., unnormalized scores) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the MNIST dataset using tensorflow_datasets\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MNIST dataset using tensorflow_datasets\n",
    "mnist_data, info = tfds.load('mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# Prepare the training and test sets\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "\n",
    "# Normalize the data\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize the image to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Map the preprocessing function and batch the data\n",
    "train_data = train_data.map(preprocess).batch(128).shuffle(60000).repeat()\n",
    "test_data = test_data.map(preprocess).batch(128)\n",
    "\n",
    "# Define the three-layer neural network model without using Keras\n",
    "class ThreeLayerNN(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases\n",
    "        self.w1 = tf.Variable(tf.random.normal([784, 128]))  # Weights for the first layer\n",
    "        self.b1 = tf.Variable(tf.zeros([128]))  # Biases for the first layer\n",
    "        self.w2 = tf.Variable(tf.random.normal([128, 10]))  # Weights for the second layer (output layer)\n",
    "        self.b2 = tf.Variable(tf.zeros([10]))  # Biases for the second layer\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Feed-forward process: Apply the layers and activation functions\n",
    "        x = tf.reshape(x, [-1, 784])  # Flatten input to 1D vector (28x28 = 784)\n",
    "        \n",
    "        # First layer (input -> hidden)\n",
    "        hidden = tf.matmul(x, self.w1) + self.b1\n",
    "        hidden = tf.nn.relu(hidden)  # ReLU activation function\n",
    "        \n",
    "        # Second layer (hidden -> output)\n",
    "        output = tf.matmul(hidden, self.w2) + self.b2\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = ThreeLayerNN()\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "# Define the optimization step (gradient descent)\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "# Training step\n",
    "def train_step(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)  # Forward pass\n",
    "        loss = compute_loss(logits, labels)  # Compute the loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)  # Backpropagation\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Update weights\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "max_steps = 6000\n",
    "steps_taken = 0\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for step, (images, labels) in enumerate(train_data):\n",
    "        if steps_taken >= max_steps:\n",
    "            print(f\"Training stopped after {steps_taken} steps.\")\n",
    "            break\n",
    "        loss = train_step(model, images, labels)\n",
    "        total_loss += loss\n",
    "        steps_taken += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / (step+1)}\")\n",
    "    if steps_taken >= max_steps:\n",
    "        break\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate(model, test_data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for images, labels in test_data:\n",
    "        logits = model(images)\n",
    "        predicted_classes = tf.argmax(logits, axis=1)\n",
    "        correct_predictions += tf.reduce_sum(tf.cast(tf.equal(predicted_classes, labels), tf.int32))\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Test Accuracy: {accuracy.numpy():.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1, Step 0, Loss: 85.91024780273438\n",
    "Epoch 1, Step 100, Loss: 21.205188751220703\n",
    "Epoch 1, Step 200, Loss: 11.716341018676758\n",
    "Epoch 1, Step 300, Loss: 9.297313690185547\n",
    "Epoch 1, Step 400, Loss: 5.473689079284668\n",
    "Epoch 1, Step 500, Loss: 4.822507858276367\n",
    "Epoch 1, Step 600, Loss: 6.340322494506836\n",
    "Epoch 1, Step 700, Loss: 2.434171676635742\n",
    "Epoch 1, Step 800, Loss: 4.003796577453613\n",
    "Epoch 1, Step 900, Loss: 4.418111801147461\n",
    "Epoch 1, Step 1000, Loss: 3.739407777786255\n",
    "Epoch 1, Step 1100, Loss: 5.287736892700195\n",
    "Epoch 1, Step 1200, Loss: 3.784698486328125\n",
    "Epoch 1, Step 1300, Loss: 3.6586251258850098\n",
    "Epoch 1, Step 1400, Loss: 3.1827216148376465\n",
    "Epoch 1, Step 1500, Loss: 2.293025493621826\n",
    "Epoch 1, Step 1600, Loss: 1.2814149856567383\n",
    "Epoch 1, Step 1700, Loss: 1.6976308822631836\n",
    "Epoch 1, Step 1800, Loss: 2.56768798828125\n",
    "Epoch 1, Step 1900, Loss: 2.1578593254089355\n",
    "Epoch 1, Step 2000, Loss: 1.1334123611450195\n",
    "Epoch 1, Step 2100, Loss: 1.1644039154052734\n",
    "Epoch 1, Step 2200, Loss: 1.4203486442565918\n",
    "Epoch 1, Step 2300, Loss: 1.768651008605957\n",
    "Epoch 1, Step 2400, Loss: 3.2192041873931885\n",
    "Epoch 1, Step 2500, Loss: 1.1171462535858154\n",
    "Epoch 1, Step 2600, Loss: 1.1473515033721924\n",
    "Epoch 1, Step 2700, Loss: 1.546579122543335\n",
    "Epoch 1, Step 2800, Loss: 0.9827357530593872\n",
    "Epoch 1, Step 2900, Loss: 1.5827875137329102\n",
    "Epoch 1, Step 3000, Loss: 1.768663763999939\n",
    "Epoch 1, Step 3100, Loss: 1.8374565839767456\n",
    "Epoch 1, Step 3200, Loss: 0.6486300230026245\n",
    "Epoch 1, Step 3300, Loss: 1.1468247175216675\n",
    "Epoch 1, Step 3400, Loss: 1.2822084426879883\n",
    "Epoch 1, Step 3500, Loss: 1.4046064615249634\n",
    "Epoch 1, Step 3600, Loss: 1.4284842014312744\n",
    "Epoch 1, Step 3700, Loss: 0.1753849983215332\n",
    "Epoch 1, Step 3800, Loss: 0.9920656085014343\n",
    "Epoch 1, Step 3900, Loss: 0.5509939193725586\n",
    "Epoch 1, Step 4000, Loss: 0.7818695306777954\n",
    "Epoch 1, Step 4100, Loss: 0.6392533779144287\n",
    "Epoch 1, Step 4200, Loss: 0.758773922920227\n",
    "Epoch 1, Step 4300, Loss: 0.8847752809524536\n",
    "Epoch 1, Step 4400, Loss: 0.5870991349220276\n",
    "Epoch 1, Step 4500, Loss: 0.5910606384277344\n",
    "Epoch 1, Step 4600, Loss: 1.1088502407073975\n",
    "Epoch 1, Step 4700, Loss: 0.5543816685676575\n",
    "Epoch 1, Step 4800, Loss: 0.2031136453151703\n",
    "Epoch 1, Step 4900, Loss: 0.8267395496368408\n",
    "Epoch 1, Step 5000, Loss: 0.7878556847572327\n",
    "Epoch 1, Step 5100, Loss: 0.3832971751689911\n",
    "Epoch 1, Step 5200, Loss: 0.41853636503219604\n",
    "Epoch 1, Step 5300, Loss: 0.9707202315330505\n",
    "Epoch 1, Step 5400, Loss: 0.1916409134864807\n",
    "Epoch 1, Step 5500, Loss: 1.0163875818252563\n",
    "Epoch 1, Step 5600, Loss: 0.40427395701408386\n",
    "Epoch 1, Step 5700, Loss: 0.24393054842948914\n",
    "Epoch 1, Step 5800, Loss: 0.27459725737571716\n",
    "Epoch 1, Step 5900, Loss: 0.4308617115020752\n",
    "Training stopped after 6000 steps.\n",
    "Epoch 1, Average Loss: 2.770639181137085\n",
    "Test Accuracy: 0.9343"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
